1:"$Sreact.fragment"
2:I[87668,["/_next/static/chunks/855f09ec501ade47.js","/_next/static/chunks/3008efbe0722a388.js"],"SiteHeader"]
3:I[81204,["/_next/static/chunks/855f09ec501ade47.js","/_next/static/chunks/3008efbe0722a388.js"],"PageTransition"]
4:I[39756,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
5:I[37457,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
6:I[33517,["/_next/static/chunks/855f09ec501ade47.js","/_next/static/chunks/3008efbe0722a388.js","/_next/static/chunks/8c5b2dd970729943.js"],"SectionReveal"]
7:I[21577,["/_next/static/chunks/855f09ec501ade47.js","/_next/static/chunks/3008efbe0722a388.js","/_next/static/chunks/8c5b2dd970729943.js"],"HeroStats"]
11:I[68027,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"default"]
:HL["/_next/static/chunks/50b97dab90896fa9.css","style"]
:HL["/_next/static/media/797e433ab948586e-s.p.dbea232f.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/caa3a2e1cccd8315-s.p.853070df.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
0:{"P":null,"b":"k4BAtFLUu7yb-Evz-1Ie7","c":["",""],"q":"","i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/50b97dab90896fa9.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/855f09ec501ade47.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/_next/static/chunks/3008efbe0722a388.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"dark","children":["$","body",null,{"className":"geist_a71539c9-module__T19VSG__variable geist_mono_8d43a2aa-module__8Li5zG__variable antialiased bg-bg-primary text-text-primary","children":[["$","$L2",null,{}],["$","$L3",null,{"children":["$","main",null,{"className":"pt-16 min-h-screen","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}],["$","footer",null,{"className":"mt-24 relative","children":[["$","div",null,{"className":"h-px bg-gradient-to-r from-transparent via-border/50 to-transparent"}],["$","div",null,{"className":"bg-bg-section-alt/50","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 py-10 flex flex-col sm:flex-row items-center justify-between gap-4","children":[["$","div",null,{"children":[["$","p",null,{"className":"text-sm font-semibold text-text-secondary","children":"QuantiBench"}],["$","p",null,{"className":"text-xs text-text-muted mt-1","children":"Open-source LLM quantization benchmarks"}]]}],["$","p",null,{"className":"text-xs text-text-muted","children":"Data is open-source. Built for the local LLM community."}]]}]}]]}]]}]}]]}],{"children":[["$","$1","c",{"children":[["$","div",null,{"children":[["$","div",null,{"className":"mx-auto max-w-7xl px-6","children":["$","$L6",null,{"children":["$","section",null,{"className":"py-24 md:py-32 text-center relative","children":[["$","div",null,{"className":"absolute inset-0 bg-[radial-gradient(ellipse_60%_40%_at_50%_40%,rgba(96,165,250,0.04),transparent_70%)]"}],["$","h1",null,{"className":"text-6xl md:text-8xl font-bold tracking-tight text-metallic mb-6 relative","style":{"textShadow":"0 0 40px rgba(255,255,255,0.1)"},"children":"QuantiBench"}],["$","p",null,{"className":"text-xl md:text-2xl text-zinc-300 mb-6 max-w-2xl mx-auto relative","children":"Does quantization harm LLM performance? We measured it."}],["$","p",null,{"className":"text-base text-text-muted max-w-3xl mx-auto leading-relaxed relative mb-12","children":"We benchmarked 5 popular open-source LLMs at 8 quantization levels across 5 rigorous evaluation suites, measuring exactly how much quality you keep — and what you trade for speed and size."}],["$","$L7",null,{"stats":[{"value":5,"label":"Models","suffix":""},{"value":8,"label":"Quant Levels","suffix":""},{"value":5,"label":"Benchmarks","suffix":""},{"value":40,"label":"Data Points","suffix":"+"}]}]]}]}]}],["$","div",null,{"className":"mx-auto max-w-7xl px-6","children":["$","$L6",null,{"children":["$","section",null,{"className":"py-16","children":[["$","h2",null,{"className":"section-heading text-2xl md:text-3xl font-bold text-text-primary mb-2","children":"Average Quality Retention"}],"$L8","$L9"]}]}]}],"$La","$Lb","$Lc","$Ld"]}],["$Le"],"$Lf"]}],{},null,false,false]},null,false,false],"$L10",false]],"m":"$undefined","G":["$11",[]],"S":true}
12:I[10765,["/_next/static/chunks/855f09ec501ade47.js","/_next/static/chunks/3008efbe0722a388.js","/_next/static/chunks/8c5b2dd970729943.js"],"OverallRetentionBar"]
13:I[83041,["/_next/static/chunks/855f09ec501ade47.js","/_next/static/chunks/3008efbe0722a388.js","/_next/static/chunks/8c5b2dd970729943.js"],"BenchmarkGrid"]
14:I[22102,["/_next/static/chunks/855f09ec501ade47.js","/_next/static/chunks/3008efbe0722a388.js","/_next/static/chunks/8c5b2dd970729943.js"],"SpeedQualityScatter"]
15:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"OutletBoundary"]
16:"$Sreact.suspense"
18:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"ViewportBoundary"]
1a:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"MetadataBoundary"]
8:["$","p",null,{"className":"text-text-muted mb-8 pl-4","children":"How much quality do you keep at each quantization level? Averaged across all models and benchmarks."}]
9:["$","div",null,{"className":"glass-card p-8","children":["$","$L12",null,{"data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.7},{"quant":"Q6_K","retention":96.9},{"quant":"Q5_K_M","retention":95.7},{"quant":"Q4_K_M","retention":93.2},{"quant":"Q3_K_M","retention":87.5},{"quant":"IQ2_XXS","retention":76.5},{"quant":"IQ1_S","retention":64.3}]}]}]
a:["$","div",null,{"className":"section-divider"}]
b:["$","div",null,{"className":"section-alt","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 relative","children":["$","$L6",null,{"children":["$","section",null,{"className":"py-16","children":[["$","h2",null,{"className":"section-heading text-2xl md:text-3xl font-bold text-text-primary mb-2","children":"Retention by Benchmark"}],["$","p",null,{"className":"text-text-muted mb-8 pl-4","children":"Quality retention varies by task type. Some benchmarks are more sensitive to quantization than others."}],["$","$L13",null,{"benchmarks":[{"key":"ifeval","name":"IFEval","description":"Instruction following","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.5},{"quant":"Q6_K","retention":97.2},{"quant":"Q5_K_M","retention":95.8},{"quant":"Q4_K_M","retention":92.9},{"quant":"Q3_K_M","retention":87.6},{"quant":"IQ2_XXS","retention":76.6},{"quant":"IQ1_S","retention":61.3}]},{"key":"bbh","name":"BBH","description":"Complex reasoning","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.7},{"quant":"Q6_K","retention":96.5},{"quant":"Q5_K_M","retention":95.7},{"quant":"Q4_K_M","retention":93},{"quant":"Q3_K_M","retention":87.6},{"quant":"IQ2_XXS","retention":76.4},{"quant":"IQ1_S","retention":64.6}]},{"key":"gpqa","name":"GPQA Diamond","description":"Graduate-level science","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.9},{"quant":"Q6_K","retention":96.9},{"quant":"Q5_K_M","retention":95.5},{"quant":"Q4_K_M","retention":94.3},{"quant":"Q3_K_M","retention":87.8},{"quant":"IQ2_XXS","retention":74.3},{"quant":"IQ1_S","retention":66.1}]},{"key":"musr","name":"MuSR","description":"Multistep reasoning","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.7},{"quant":"Q6_K","retention":97},{"quant":"Q5_K_M","retention":95.5},{"quant":"Q4_K_M","retention":93.4},{"quant":"Q3_K_M","retention":86.9},{"quant":"IQ2_XXS","retention":77.9},{"quant":"IQ1_S","retention":63.5}]},{"key":"hle","name":"HLE","description":"Expert-level ceiling","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.7},{"quant":"Q6_K","retention":96.7},{"quant":"Q5_K_M","retention":96},{"quant":"Q4_K_M","retention":92.4},{"quant":"Q3_K_M","retention":87.8},{"quant":"IQ2_XXS","retention":77.2},{"quant":"IQ1_S","retention":65.9}]}]}]]}]}]}]}]
c:["$","div",null,{"className":"section-divider"}]
d:["$","div",null,{"className":"mx-auto max-w-7xl px-6","children":["$","$L6",null,{"children":["$","section",null,{"className":"py-16","children":[["$","h2",null,{"className":"section-heading text-2xl md:text-3xl font-bold text-text-primary mb-2","children":"Speed vs Quality"}],["$","p",null,{"className":"text-text-muted mb-8 pl-4","children":"The tradeoff visualized. Each dot is one model at one quantization level. Dot size reflects VRAM usage."}],["$","div",null,{"className":"glass-card p-8","children":["$","$L14",null,{"data":[{"modelName":"Llama 3.1 8B Instruct","quant":"FP16","retention":100,"decodeToksPerSec":28.7,"vramGb":18.59},{"modelName":"Llama 3.1 8B Instruct","quant":"Q8_0","retention":98.8,"decodeToksPerSec":40.4,"vramGb":9.69},{"modelName":"Llama 3.1 8B Instruct","quant":"Q6_K","retention":96.6,"decodeToksPerSec":46.4,"vramGb":7.71},{"modelName":"Llama 3.1 8B Instruct","quant":"Q5_K_M","retention":95.4,"decodeToksPerSec":51.5,"vramGb":6.68},{"modelName":"Llama 3.1 8B Instruct","quant":"Q4_K_M","retention":93.2,"decodeToksPerSec":59.5,"vramGb":5.63},{"modelName":"Llama 3.1 8B Instruct","quant":"Q3_K_M","retention":87,"decodeToksPerSec":64.9,"vramGb":4.46},{"modelName":"Llama 3.1 8B Instruct","quant":"IQ2_XXS","retention":76.8,"decodeToksPerSec":77.9,"vramGb":3.22},{"modelName":"Llama 3.1 8B Instruct","quant":"IQ1_S","retention":64.2,"decodeToksPerSec":91.8,"vramGb":2.41},{"modelName":"Qwen 2.5 7B Instruct","quant":"FP16","retention":100,"decodeToksPerSec":30.7,"vramGb":16.73},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q8_0","retention":98.6,"decodeToksPerSec":47,"vramGb":8.62},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q6_K","retention":97.2,"decodeToksPerSec":52.6,"vramGb":6.89},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q5_K_M","retention":96,"decodeToksPerSec":58.1,"vramGb":5.96},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q4_K_M","retention":92.6,"decodeToksPerSec":63.9,"vramGb":4.95},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q3_K_M","retention":87.3,"decodeToksPerSec":74.8,"vramGb":3.95},{"modelName":"Qwen 2.5 7B Instruct","quant":"IQ2_XXS","retention":78.3,"decodeToksPerSec":88.7,"vramGb":2.77},{"modelName":"Qwen 2.5 7B Instruct","quant":"IQ1_S","retention":63.3,"decodeToksPerSec":100.5,"vramGb":2.14},{"modelName":"Mistral 7B v0.3 Instruct","quant":"FP16","retention":100,"decodeToksPerSec":31.5,"vramGb":16.7},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q8_0","retention":98.7,"decodeToksPerSec":40.6,"vramGb":8.86},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q6_K","retention":97.1,"decodeToksPerSec":49,"vramGb":6.88},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q5_K_M","retention":95.2,"decodeToksPerSec":55.7,"vramGb":6.03},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q4_K_M","retention":94,"decodeToksPerSec":65.3,"vramGb":4.99},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q3_K_M","retention":88.5,"decodeToksPerSec":75.6,"vramGb":4.07},{"modelName":"Mistral 7B v0.3 Instruct","quant":"IQ2_XXS","retention":75.3,"decodeToksPerSec":86.5,"vramGb":2.82},{"modelName":"Mistral 7B v0.3 Instruct","quant":"IQ1_S","retention":64.4,"decodeToksPerSec":97.3,"vramGb":2.19},{"modelName":"Gemma 2 9B Instruct","quant":"FP16","retention":100,"decodeToksPerSec":24,"vramGb":21.35},{"modelName":"Gemma 2 9B Instruct","quant":"Q8_0","retention":98.6,"decodeToksPerSec":34.9,"vramGb":11.48},{"modelName":"Gemma 2 9B Instruct","quant":"Q6_K","retention":96.8,"decodeToksPerSec":39.4,"vramGb":8.67},{"modelName":"Gemma 2 9B Instruct","quant":"Q5_K_M","retention":95.7,"decodeToksPerSec":45.6,"vramGb":7.7},{"modelName":"Gemma 2 9B Instruct","quant":"Q4_K_M","retention":93.6,"decodeToksPerSec":53.1,"vramGb":6.35},{"modelName":"Gemma 2 9B Instruct","quant":"Q3_K_M","retention":88.2,"decodeToksPerSec":59.9,"vramGb":5.12},{"modelName":"Gemma 2 9B Instruct","quant":"IQ2_XXS","retention":75.7,"decodeToksPerSec":70.6,"vramGb":3.66},{"modelName":"Gemma 2 9B Instruct","quant":"IQ1_S","retention":65.5,"decodeToksPerSec":83.9,"vramGb":2.77},{"modelName":"Phi-4","quant":"FP16","retention":100,"decodeToksPerSec":18.9,"vramGb":31.64},{"modelName":"Phi-4","quant":"Q8_0","retention":98.7,"decodeToksPerSec":26.1,"vramGb":17.05},{"modelName":"Phi-4","quant":"Q6_K","retention":96.6,"decodeToksPerSec":28.9,"vramGb":13.18},{"modelName":"Phi-4","quant":"Q5_K_M","retention":96.1,"decodeToksPerSec":33.8,"vramGb":11.34},{"modelName":"Phi-4","quant":"Q4_K_M","retention":92.6,"decodeToksPerSec":37.9,"vramGb":9.41},{"modelName":"Phi-4","quant":"Q3_K_M","retention":86.7,"decodeToksPerSec":43.3,"vramGb":7.75},{"modelName":"Phi-4","quant":"IQ2_XXS","retention":76.3,"decodeToksPerSec":51.3,"vramGb":5.37},{"modelName":"Phi-4","quant":"IQ1_S","retention":63.9,"decodeToksPerSec":59.1,"vramGb":4.14}]}]}]]}]}]}]
e:["$","script","script-0",{"src":"/_next/static/chunks/8c5b2dd970729943.js","async":true,"nonce":"$undefined"}]
f:["$","$L15",null,{"children":["$","$16",null,{"name":"Next.MetadataOutlet","children":"$@17"}]}]
10:["$","$1","h",{"children":[null,["$","$L18",null,{"children":"$L19"}],["$","div",null,{"hidden":true,"children":["$","$L1a",null,{"children":["$","$16",null,{"name":"Next.Metadata","children":"$L1b"}]}]}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}]
19:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1c:I[27201,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"IconMark"]
17:null
1b:[["$","title","0",{"children":"QuantiBench — LLM Quantization Benchmarks"}],["$","meta","1",{"name":"description","content":"We benchmarked 5 popular open-source LLMs at 8 quantization levels across 5 rigorous evaluation suites, measuring exactly how much quality you keep — and what you trade for speed and size."}],["$","meta","2",{"property":"og:title","content":"QuantiBench — LLM Quantization Benchmarks"}],["$","meta","3",{"property":"og:description","content":"Does quantization harm LLM performance? We measured it."}],["$","meta","4",{"property":"og:url","content":"https://quantibench.ai"}],["$","meta","5",{"property":"og:site_name","content":"QuantiBench"}],["$","meta","6",{"property":"og:type","content":"website"}],["$","meta","7",{"name":"twitter:card","content":"summary"}],["$","meta","8",{"name":"twitter:title","content":"QuantiBench — LLM Quantization Benchmarks"}],["$","meta","9",{"name":"twitter:description","content":"Does quantization harm LLM performance? We measured it."}],["$","link","10",{"rel":"icon","href":"/favicon.svg"}],["$","$L1c","11",{}]]
