1:"$Sreact.fragment"
2:I[33517,["/quantibench/_next/static/chunks/855f09ec501ade47.js","/quantibench/_next/static/chunks/3008efbe0722a388.js","/quantibench/_next/static/chunks/8c5b2dd970729943.js"],"SectionReveal"]
3:I[21577,["/quantibench/_next/static/chunks/855f09ec501ade47.js","/quantibench/_next/static/chunks/3008efbe0722a388.js","/quantibench/_next/static/chunks/8c5b2dd970729943.js"],"HeroStats"]
4:I[10765,["/quantibench/_next/static/chunks/855f09ec501ade47.js","/quantibench/_next/static/chunks/3008efbe0722a388.js","/quantibench/_next/static/chunks/8c5b2dd970729943.js"],"OverallRetentionBar"]
5:I[83041,["/quantibench/_next/static/chunks/855f09ec501ade47.js","/quantibench/_next/static/chunks/3008efbe0722a388.js","/quantibench/_next/static/chunks/8c5b2dd970729943.js"],"BenchmarkGrid"]
b:I[22102,["/quantibench/_next/static/chunks/855f09ec501ade47.js","/quantibench/_next/static/chunks/3008efbe0722a388.js","/quantibench/_next/static/chunks/8c5b2dd970729943.js"],"SpeedQualityScatter"]
c:I[97367,["/quantibench/_next/static/chunks/ff1a16fafef87110.js","/quantibench/_next/static/chunks/d2be314c3ece3fbe.js"],"OutletBoundary"]
d:"$Sreact.suspense"
0:{"buildId":"JMB6p3sw99J81YInPv3ZR","rsc":["$","$1","c",{"children":[["$","div",null,{"children":[["$","div",null,{"className":"mx-auto max-w-7xl px-6","children":["$","$L2",null,{"children":["$","section",null,{"className":"py-24 md:py-32 text-center relative","children":[["$","div",null,{"className":"absolute inset-0 bg-[radial-gradient(ellipse_60%_40%_at_50%_40%,rgba(96,165,250,0.04),transparent_70%)]"}],["$","h1",null,{"className":"text-6xl md:text-8xl font-bold tracking-tight text-metallic mb-6 relative","style":{"textShadow":"0 0 40px rgba(255,255,255,0.1)"},"children":"QuantiBench"}],["$","p",null,{"className":"text-xl md:text-2xl text-zinc-300 mb-6 max-w-2xl mx-auto relative","children":"Does quantization harm LLM performance? We measured it."}],["$","p",null,{"className":"text-base text-text-muted max-w-3xl mx-auto leading-relaxed relative mb-12","children":"We benchmarked 5 popular open-source LLMs at 8 quantization levels across 5 rigorous evaluation suites, measuring exactly how much quality you keep â€” and what you trade for speed and size."}],["$","$L3",null,{"stats":[{"value":5,"label":"Models","suffix":""},{"value":8,"label":"Quant Levels","suffix":""},{"value":5,"label":"Benchmarks","suffix":""},{"value":40,"label":"Data Points","suffix":"+"}]}]]}]}]}],["$","div",null,{"className":"mx-auto max-w-7xl px-6","children":["$","$L2",null,{"children":["$","section",null,{"className":"py-16","children":[["$","h2",null,{"className":"section-heading text-2xl md:text-3xl font-bold text-text-primary mb-2","children":"Average Quality Retention"}],["$","p",null,{"className":"text-text-muted mb-8 pl-4","children":"How much quality do you keep at each quantization level? Averaged across all models and benchmarks."}],["$","div",null,{"className":"glass-card p-8","children":["$","$L4",null,{"data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.7},{"quant":"Q6_K","retention":96.9},{"quant":"Q5_K_M","retention":95.7},{"quant":"Q4_K_M","retention":93.2},{"quant":"Q3_K_M","retention":87.5},{"quant":"IQ2_XXS","retention":76.5},{"quant":"IQ1_S","retention":64.3}]}]}]]}]}]}],["$","div",null,{"className":"section-divider"}],["$","div",null,{"className":"section-alt","children":["$","div",null,{"className":"mx-auto max-w-7xl px-6 relative","children":["$","$L2",null,{"children":["$","section",null,{"className":"py-16","children":[["$","h2",null,{"className":"section-heading text-2xl md:text-3xl font-bold text-text-primary mb-2","children":"Retention by Benchmark"}],["$","p",null,{"className":"text-text-muted mb-8 pl-4","children":"Quality retention varies by task type. Some benchmarks are more sensitive to quantization than others."}],["$","$L5",null,{"benchmarks":[{"key":"ifeval","name":"IFEval","description":"Instruction following","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.5},{"quant":"Q6_K","retention":97.2},{"quant":"Q5_K_M","retention":95.8},{"quant":"Q4_K_M","retention":92.9},{"quant":"Q3_K_M","retention":87.6},{"quant":"IQ2_XXS","retention":76.6},{"quant":"IQ1_S","retention":61.3}]},{"key":"bbh","name":"BBH","description":"Complex reasoning","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.7},{"quant":"Q6_K","retention":96.5},{"quant":"Q5_K_M","retention":95.7},{"quant":"Q4_K_M","retention":93},{"quant":"Q3_K_M","retention":87.6},{"quant":"IQ2_XXS","retention":76.4},{"quant":"IQ1_S","retention":64.6}]},{"key":"gpqa","name":"GPQA Diamond","description":"Graduate-level science","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.9},{"quant":"Q6_K","retention":96.9},{"quant":"Q5_K_M","retention":95.5},{"quant":"Q4_K_M","retention":94.3},{"quant":"Q3_K_M","retention":87.8},{"quant":"IQ2_XXS","retention":74.3},{"quant":"IQ1_S","retention":66.1}]},{"key":"musr","name":"MuSR","description":"Multistep reasoning","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.7},{"quant":"Q6_K","retention":97},{"quant":"Q5_K_M","retention":95.5},{"quant":"Q4_K_M","retention":93.4},{"quant":"Q3_K_M","retention":86.9},{"quant":"IQ2_XXS","retention":77.9},{"quant":"IQ1_S","retention":63.5}]},{"key":"hle","name":"HLE","description":"Expert-level ceiling","data":[{"quant":"FP16","retention":100},{"quant":"Q8_0","retention":98.7},{"quant":"Q6_K","retention":96.7},{"quant":"Q5_K_M","retention":96},{"quant":"Q4_K_M","retention":92.4},{"quant":"Q3_K_M","retention":87.8},{"quant":"IQ2_XXS","retention":77.2},{"quant":"IQ1_S","retention":65.9}]}]}]]}]}]}]}],["$","div",null,{"className":"section-divider"}],["$","div",null,{"className":"mx-auto max-w-7xl px-6","children":["$","$L2",null,{"children":["$","section",null,{"className":"py-16","children":["$L6","$L7","$L8"]}]}]}]]}],["$L9"],"$La"]}],"loading":null,"isPartial":false}
6:["$","h2",null,{"className":"section-heading text-2xl md:text-3xl font-bold text-text-primary mb-2","children":"Speed vs Quality"}]
7:["$","p",null,{"className":"text-text-muted mb-8 pl-4","children":"The tradeoff visualized. Each dot is one model at one quantization level. Dot size reflects VRAM usage."}]
8:["$","div",null,{"className":"glass-card p-8","children":["$","$Lb",null,{"data":[{"modelName":"Llama 3.1 8B Instruct","quant":"FP16","retention":100,"decodeToksPerSec":28.7,"vramGb":18.59},{"modelName":"Llama 3.1 8B Instruct","quant":"Q8_0","retention":98.8,"decodeToksPerSec":40.4,"vramGb":9.69},{"modelName":"Llama 3.1 8B Instruct","quant":"Q6_K","retention":96.6,"decodeToksPerSec":46.4,"vramGb":7.71},{"modelName":"Llama 3.1 8B Instruct","quant":"Q5_K_M","retention":95.4,"decodeToksPerSec":51.5,"vramGb":6.68},{"modelName":"Llama 3.1 8B Instruct","quant":"Q4_K_M","retention":93.2,"decodeToksPerSec":59.5,"vramGb":5.63},{"modelName":"Llama 3.1 8B Instruct","quant":"Q3_K_M","retention":87,"decodeToksPerSec":64.9,"vramGb":4.46},{"modelName":"Llama 3.1 8B Instruct","quant":"IQ2_XXS","retention":76.8,"decodeToksPerSec":77.9,"vramGb":3.22},{"modelName":"Llama 3.1 8B Instruct","quant":"IQ1_S","retention":64.2,"decodeToksPerSec":91.8,"vramGb":2.41},{"modelName":"Qwen 2.5 7B Instruct","quant":"FP16","retention":100,"decodeToksPerSec":30.7,"vramGb":16.73},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q8_0","retention":98.6,"decodeToksPerSec":47,"vramGb":8.62},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q6_K","retention":97.2,"decodeToksPerSec":52.6,"vramGb":6.89},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q5_K_M","retention":96,"decodeToksPerSec":58.1,"vramGb":5.96},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q4_K_M","retention":92.6,"decodeToksPerSec":63.9,"vramGb":4.95},{"modelName":"Qwen 2.5 7B Instruct","quant":"Q3_K_M","retention":87.3,"decodeToksPerSec":74.8,"vramGb":3.95},{"modelName":"Qwen 2.5 7B Instruct","quant":"IQ2_XXS","retention":78.3,"decodeToksPerSec":88.7,"vramGb":2.77},{"modelName":"Qwen 2.5 7B Instruct","quant":"IQ1_S","retention":63.3,"decodeToksPerSec":100.5,"vramGb":2.14},{"modelName":"Mistral 7B v0.3 Instruct","quant":"FP16","retention":100,"decodeToksPerSec":31.5,"vramGb":16.7},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q8_0","retention":98.7,"decodeToksPerSec":40.6,"vramGb":8.86},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q6_K","retention":97.1,"decodeToksPerSec":49,"vramGb":6.88},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q5_K_M","retention":95.2,"decodeToksPerSec":55.7,"vramGb":6.03},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q4_K_M","retention":94,"decodeToksPerSec":65.3,"vramGb":4.99},{"modelName":"Mistral 7B v0.3 Instruct","quant":"Q3_K_M","retention":88.5,"decodeToksPerSec":75.6,"vramGb":4.07},{"modelName":"Mistral 7B v0.3 Instruct","quant":"IQ2_XXS","retention":75.3,"decodeToksPerSec":86.5,"vramGb":2.82},{"modelName":"Mistral 7B v0.3 Instruct","quant":"IQ1_S","retention":64.4,"decodeToksPerSec":97.3,"vramGb":2.19},{"modelName":"Gemma 2 9B Instruct","quant":"FP16","retention":100,"decodeToksPerSec":24,"vramGb":21.35},{"modelName":"Gemma 2 9B Instruct","quant":"Q8_0","retention":98.6,"decodeToksPerSec":34.9,"vramGb":11.48},{"modelName":"Gemma 2 9B Instruct","quant":"Q6_K","retention":96.8,"decodeToksPerSec":39.4,"vramGb":8.67},{"modelName":"Gemma 2 9B Instruct","quant":"Q5_K_M","retention":95.7,"decodeToksPerSec":45.6,"vramGb":7.7},{"modelName":"Gemma 2 9B Instruct","quant":"Q4_K_M","retention":93.6,"decodeToksPerSec":53.1,"vramGb":6.35},{"modelName":"Gemma 2 9B Instruct","quant":"Q3_K_M","retention":88.2,"decodeToksPerSec":59.9,"vramGb":5.12},{"modelName":"Gemma 2 9B Instruct","quant":"IQ2_XXS","retention":75.7,"decodeToksPerSec":70.6,"vramGb":3.66},{"modelName":"Gemma 2 9B Instruct","quant":"IQ1_S","retention":65.5,"decodeToksPerSec":83.9,"vramGb":2.77},{"modelName":"Phi-4","quant":"FP16","retention":100,"decodeToksPerSec":18.9,"vramGb":31.64},{"modelName":"Phi-4","quant":"Q8_0","retention":98.7,"decodeToksPerSec":26.1,"vramGb":17.05},{"modelName":"Phi-4","quant":"Q6_K","retention":96.6,"decodeToksPerSec":28.9,"vramGb":13.18},{"modelName":"Phi-4","quant":"Q5_K_M","retention":96.1,"decodeToksPerSec":33.8,"vramGb":11.34},{"modelName":"Phi-4","quant":"Q4_K_M","retention":92.6,"decodeToksPerSec":37.9,"vramGb":9.41},{"modelName":"Phi-4","quant":"Q3_K_M","retention":86.7,"decodeToksPerSec":43.3,"vramGb":7.75},{"modelName":"Phi-4","quant":"IQ2_XXS","retention":76.3,"decodeToksPerSec":51.3,"vramGb":5.37},{"modelName":"Phi-4","quant":"IQ1_S","retention":63.9,"decodeToksPerSec":59.1,"vramGb":4.14}]}]}]
9:["$","script","script-0",{"src":"/quantibench/_next/static/chunks/8c5b2dd970729943.js","async":true}]
a:["$","$Lc",null,{"children":["$","$d",null,{"name":"Next.MetadataOutlet","children":"$@e"}]}]
e:null
